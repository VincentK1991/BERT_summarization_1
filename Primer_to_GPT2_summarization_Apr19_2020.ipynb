{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Primer_to_GPT2_summarization_Apr19_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMlRyXIVt5OiyICBrFSFy+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentK1991/BERT_summarization_1/blob/master/Primer_to_GPT2_summarization_Apr19_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHo7WRJrk0Tb",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Abstractive summarization is a task of shortening a text by paraphrasing. Unlike extractive summarization where a \"key\" sentence is extracted from a paragraph, abstractive summarization relies on language modeling task, i.e.  predict the next workds/sentences.\n",
        "\n",
        "Re-framing the question is way, our task becomes how to use languge model to generate a summary given the main text.\n",
        "\n",
        "$TEXT => summary$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmdGmo3N6eMH",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.topbots.com/wp-content/uploads/2019/04/language-model-comparison_web.jpg)\n",
        "\n",
        "We will use Open-AI GPT2 to do this task. Unlike BERT which are bidirectional encoder, GPT2 is auto-regressive. It means the output is fed back in as a new input, like the recurrent-neural network. The information flows to the right only. This has **advantage and drawback**. Comparison with BERT is [here](https://www.topbots.com/generalized-language-models-bert-openai-gpt2/).\n",
        "\n",
        "The advantage is that it is good at generating new words on its own. i.e. paraphrasing, conversing, or the like; whereas BERT is very good at classification or finding highlight in a text.\n",
        "\n",
        "The drawback is that feeding output back as input can generate a vicious loop or getting off topic. For example, sometimes the output keeps repeating itself (i.e. the output feeding back to the model creating a recursive loop), or output slowly drifting to a new topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUVwnLqT6hi2",
        "colab_type": "text"
      },
      "source": [
        "GPT2 uses decoder stacks to do language model. Each stack is called \"transformer block\". The lite version of GPT2 that we use called \"distilgpt2\" has 6 blocks. The typical version (small) has 12 blocks.\n",
        "\n",
        "But first of it starts with embedding, so this is just to embed our tokenized words to high dimensional space. There are two kinds, \"WTE\" for \"word-token embedding\" and \"WPE\" for \"word-position embedding\". The \"WTE\" maps 50K vocab to 768 dimensions. The \"WPE\" maps word position up to 1024 in a text to 768 dimensions.\n",
        "\n",
        "            (wte): Embedding(50257, 768)\n",
        "            (wpe): Embedding(1024, 768)\n",
        "            (drop): Dropout(p=0.1, inplace=False)\n",
        "\n",
        "Each block has these stacks:\n",
        "\n",
        "          (0): Block(\n",
        "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (attn): Attention(\n",
        "            (c_attn): Conv1D()\n",
        "            (c_proj): Conv1D()\n",
        "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
        "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (mlp): MLP(\n",
        "            (c_fc): Conv1D()\n",
        "            (c_proj): Conv1D()\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "          )\n",
        "Basically, we have 6 of these blocks on top of one another.\n",
        "\n",
        "At the end, we have \n",
        "\n",
        "          (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
        "          (lm_head): Linear(in_features=768, out_features=50257,\n",
        "           bias=False)\n",
        "\n",
        "Which maps the high dimensional space back to tokenized words.\n",
        "\n",
        "The key to success seems to be the use of clever attention mechanism. read more [here](http://jalammar.github.io/illustrated-gpt2/) or [here](https://github.com/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0JDr1i9AW5b",
        "colab_type": "text"
      },
      "source": [
        "The training of this kind of model works like most other neural network model. [Under the hood](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), the model performs forward propagation, computes loss, performs backward propagation, performs step optimization, clip gradient norm, and repeat.\n",
        "\n",
        "The loss is called \"causal language modeling\" (CLM) loss, which is a cross entropy loss where the labels are input shifted to the right by 1 tokens. \n",
        "\n",
        "The exponentiated form of the loss is also called \"perplexity\", meaning how \"perplexed\" the model is to see the answer, not surprise = low score = good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHjC9f-wposn",
        "colab_type": "text"
      },
      "source": [
        "# set up packages\n",
        "\n",
        "This work will be pytorch-based. And use adaptation developed by [Huggingface](https://huggingface.co/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWmZ7EpBpfhJ",
        "colab_type": "code",
        "outputId": "e245b5a8-dc74-4b19-81a2-ee3a3190cdb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkUrPNEUqhzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict_to_obj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izSZ1e7drQvQ",
        "colab_type": "code",
        "outputId": "52e11418-9547-4960-dae6-2b4829d5545e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7in4-STgrTaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import run_language_modeling  # package from huggingface\n",
        "import run_generation # package from huggingface\n",
        "from dict_to_obj import DictToObj\n",
        "import collections\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hp8SgrFuFNb",
        "colab_type": "code",
        "outputId": "c4f083bb-0b86-45be-cae5-bee443499516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2eFXGXfkaOh",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning the model\n",
        "\n",
        "I use 100K samples (about 300MB of txt file) to train on GPU. I planned to run for ~ 5 epochs, but one epoch already takes about 2 hours. So I just run 1 epoch.\n",
        "\n",
        "I use 100K samples for evaluation, which is also overkilled. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVCkALrKrVf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/Newsroom' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=distilgpt2 \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=7.0 \\\n",
        "    --do_train \\\n",
        "    --overwrite_output_dir \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=5000 \\\n",
        "    --save_steps=5000 \\\n",
        "    --train_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/trainFile_small_format_Apr19_2020.txt' \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/devFile_format_Apr19_2020.txt' \\\n",
        "    --per_gpu_train_batch_size=256 \\\n",
        "    --per_gpu_eval_batch_size=256 \\\n",
        "    --block_size=5 \\\n",
        "    --gradient_accumulation_steps=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ptM-VJqQVR",
        "colab_type": "code",
        "outputId": "8d74f43c-75bd-4ff3-8992-0800af6f27ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/finetuned_models/Newsroom'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint-10000  checkpoint-25000    test_eval_checkpoint-5000\n",
            "checkpoint-15000  checkpoint-5000\n",
            "checkpoint-20000  eval_results_1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ole7QuFkmDD",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XliRkN3m0Jft",
        "colab_type": "text"
      },
      "source": [
        "run the evaluation using run_language_modeling command line directly. We will use the weights and model configuration saved in the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOkvE-dD19Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --train_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/trainFile_small_format_Apr19_2020.txt' \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path='/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000' \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/devFile_small2_format_Apr19_2020.txt' \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=128 \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtoSZ0osJsa1",
        "colab_type": "text"
      },
      "source": [
        "# generate text\n",
        "\n",
        "To do this, we will call a model and do a forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvojDIxPKDiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb4epoBDONrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROMPT = \"\"\"<|startoftext|> the precolonial era, the area of present-day New York City was inhabited by Algonquian Native Americans, including the Lenape. Their homeland, known as Lenapehoking, included Staten Island, Manhattan, the Bronx, the western portion of Long Island (including the areas that would later become the boroughs of Brooklyn and Queens), and the Lower Hudson Valley.\n",
        "The first documented visit into New York Harbor by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown. He claimed the area for France and named it Nouvelle Angoulême (New Angoulême). A Spanish expedition, led by the Portuguese captain Estêvão Gomes sailing for Emperor Charles V, arrived in New York Harbor in January 1525 and charted the mouth of the Hudson River, which he named Río de San Antonio (Saint Anthony's River). The Padrón Real of 1527, the first scientific map to show the East Coast of North America continuously, was informed by Gomes' expedition and labeled the northeastern United States as Tierra de Esteban Gómez in his honor. <|summarize|>\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RRkEM16Jxy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = run_language_modeling.AutoTokenizer.from_pretrained(CHECKPOINT_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIoY5ZPyzUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjYwQVafKGhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_prompt = tokenizer.encode(PROMPT, add_special_tokens=False, return_tensors=\"pt\")\n",
        "encoded_prompt = encoded_prompt.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8KI_9ALKnDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = run_language_modeling.AutoModelWithLMHead.from_pretrained(CHECKPOINT_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yypCpy2MXJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdSVR0HgLU58",
        "colab_type": "code",
        "outputId": "93f00901-c957-4ee7-b98f-9df55ba73ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=200 + len(encoded_prompt[0]),\n",
        "      temperature=0.5,\n",
        "      decoder_start_token_id= '<|summarize|>',\n",
        "      top_k=50,\n",
        "      top_p=1.0,\n",
        "      repetition_penalty=None,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=3)\n",
        " # Remove the batch dimension when returning multiple sequences\n",
        "if len(output_sequences.shape) > 2:\n",
        "  output_sequences.squeeze_()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2020 19:47:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zllX9tRMEgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_token = '<|endoftext|>'\n",
        "generated_sequences = []  \n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "  generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "  # Decode text\n",
        "  text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "  # Remove all text after the stop token\n",
        "  text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "  # Remove the excess text that was used for pre-processing\n",
        "  text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "  # Add the prompt at the beginning of the sequence.\n",
        "  total_sequence = PROMPT + text\n",
        "\n",
        "  generated_sequences.append(total_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehzUIPgXPfMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h9jEUYBO4Vj",
        "colab_type": "code",
        "outputId": "ad18edb3-5788-4030-9279-fad542c3f67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "wrapper.wrap(generated_sequences[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|startoftext|> the precolonial era, the area of present-day New York',\n",
              " 'City was inhabited by Algonquian Native Americans, including the',\n",
              " 'Lenape. Their homeland, known as Lenapehoking, included Staten Island,',\n",
              " 'Manhattan, the Bronx, the western portion of Long Island (including',\n",
              " 'the areas that would later become the boroughs of Brooklyn and',\n",
              " 'Queens), and the Lower Hudson Valley. The first documented visit into',\n",
              " 'New York Harbor by a European was in 1524 by Giovanni da Verrazzano, a',\n",
              " 'Florentine explorer in the service of the French crown. He claimed the',\n",
              " 'area for France and named it Nouvelle Angoulême (New Angoulême). A',\n",
              " 'Spanish expedition, led by the Portuguese captain Estêvão Gomes',\n",
              " 'sailing for Emperor Charles V, arrived in New York Harbor in January',\n",
              " '1525 and charted the mouth of the Hudson River, which he named Río de',\n",
              " \"San Antonio (Saint Anthony's River). The Padrón Real of 1527, the\",\n",
              " 'first scientific map to show the East Coast of North America',\n",
              " \"continuously, was informed by Gomes' expedition and labeled the\",\n",
              " 'northeastern United States as Tierra de Esteban Gómez in his honor.',\n",
              " '<|summarize|> and the S.Fernando, who was a little known for the first',\n",
              " \"time, the late-day, and the New York, in the late 60's, the ''The New\",\n",
              " 'York Stock Exchange, and the first and the second in the U.S.A.S. at',\n",
              " 'the time of the arrival of the late 1940s, the Dutch and the first',\n",
              " 'American colonies, the same-day, and the first time his father, the',\n",
              " \"old-flier is the first American, with a ''Ceauf, the first lady, and\",\n",
              " 'the first-in-born physician who was born in the late 19th- and 19th',\n",
              " 'century.         . \"The last year, the late-1940s and early-century,',\n",
              " 'it was the first to be a major exhibition of the city of the year, the',\n",
              " \"first to be called ''The New York, not the first to be the first to be\",\n",
              " 'published, the first i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9sa5_4eqSRr",
        "colab_type": "text"
      },
      "source": [
        "## future direction\n",
        "\n",
        "Still at this stage, the summarization looks gibberish.\n",
        "\n",
        "There are many things to try.\n",
        "For example,\n",
        "- Try changing the block_size, the first trial I use block size = 5. This is very likely a wrong choice as it is too short. Perhaps try, longer like 512, 1024 or use default by not specifying or specify as -1.\n",
        "\n",
        "- size of the dev file (initially use 100K samples, this takes too long) 10K sample would have been fine I think. \n",
        "\n",
        "- and gradient accumulation. Perhaps, if I use bigger batch size, I don't have to use this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj8oYis6lEx8",
        "colab_type": "text"
      },
      "source": [
        "# Citation\n",
        "\n",
        "[distilgpt2](https://huggingface.co/distilgpt2)\n",
        "- a lite version of GPT2, making it more compact and faster to train\n",
        "\n",
        "[example code for training GPT2](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py)\n",
        "- An example by Huggingface on how to train GPT2. In this notebook, we simply call this command directly to train GPT2.\n",
        "\n",
        "[example code for language generation](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py)\n",
        "- Example by Huggingface on how to generate text from pre-trained or fine-tuned GPT2. \n",
        "\n",
        "[Dataset](https://summari.es/)\n",
        "- We use first 100K summary-text pairs in the training set to train the model.\n",
        "- This dataset also has articles of various length, making training more challenging (!?). Other dataset that I see has articles that are shorter and more uniform in length.\n",
        "\n",
        "\n",
        "[blog post on how to train GPT2](https://minimaxir.com/2019/09/howto-gpt2/)\n",
        "\n",
        "[Annoted notbook on how attention works](https://github.com/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb)\n",
        "\n",
        "[Blog post on attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
        "\n",
        "[illutration of how GPT2 works](http://jalammar.github.io/illustrated-gpt2/)\n",
        "- I use this as a guide to strategize how to train GPT2 for abstractive summarization.\n",
        "\n",
        "- Basically from what I understand, The training set just contains pairs of article texts and their summaries, separated by a keyword token. In this case, I use '<|startoftext|>' to denote where the article text starts. '<|summarize|> ' to denote where the text ends and the summary starts. ' <|endoftext|>' to denote where the summary ends. This part is what I'm not sure about what kind of tokens I should use.  \n",
        "\n",
        "- I did not specify to the model tokenizer that they are special tokens. Perhaps this would make a big difference if I did. So, this is something to play around with. "
      ]
    }
  ]
}