{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BERTandGPT2_abstractive_summarization_Apr28_2020.ipynb",
      "provenance": [],
      "mount_file_id": "1U2mFY6vGgKWNk326_vPEQEn0an-0jtvi",
      "authorship_tag": "ABX9TyNsav91xu16gtF29LlBHw2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da8d1c53d71e44dabb130fcdb4b85253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef22ba1a71d747ccad770b27afd3a60b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab1f2ff9b3f8408b912f5bfc0b410bc2",
              "IPY_MODEL_30222c3cb41c47c9b7c0ab5aff02ff71"
            ]
          }
        },
        "ef22ba1a71d747ccad770b27afd3a60b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab1f2ff9b3f8408b912f5bfc0b410bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_16e8062727654fb2849d9c00a3a210ef",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7a8e07c934546dea428873d95cacba2"
          }
        },
        "30222c3cb41c47c9b7c0ab5aff02ff71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e89829843ba417ca66c9ed79cb75a4a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:12&lt;00:00, 36.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9eae5e0e932e4d4382916c19523b698c"
          }
        },
        "16e8062727654fb2849d9c00a3a210ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7a8e07c934546dea428873d95cacba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e89829843ba417ca66c9ed79cb75a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9eae5e0e932e4d4382916c19523b698c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ecf7dc9b01e463580e99850600f429c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4bca4ef8234a4543bd4d6f7ef127e579",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe59bb8422da43a8b46870e120638268",
              "IPY_MODEL_7257d84b48664262a1e372af2176de7c"
            ]
          }
        },
        "4bca4ef8234a4543bd4d6f7ef127e579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe59bb8422da43a8b46870e120638268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22ecbf34bf6d42b7a25f0dce0f470466",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c11cfcd199346fbb6f8cd2c25194f0b"
          }
        },
        "7257d84b48664262a1e372af2176de7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f69dc0cef880427097808227a3b68069",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:09&lt;00:00, 28.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d80540b5a04b41b8ab9c9852d15d79fb"
          }
        },
        "22ecbf34bf6d42b7a25f0dce0f470466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c11cfcd199346fbb6f8cd2c25194f0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f69dc0cef880427097808227a3b68069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d80540b5a04b41b8ab9c9852d15d79fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8be1ccca11094c5981d9765a0c0ca568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_12261b92cbbd492ab8cb585ffaaf7a84",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_73f63f91b5d04758b3160ee0e63a6db4",
              "IPY_MODEL_ac5390f0a3184b35889479d450a6a435"
            ]
          }
        },
        "12261b92cbbd492ab8cb585ffaaf7a84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73f63f91b5d04758b3160ee0e63a6db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1334e3d8d2048bead2389d0e507a44a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dc553df3b1e4ae0a58fd9cc412d7411"
          }
        },
        "ac5390f0a3184b35889479d450a6a435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_15372b33d6c04882a9740a8e8458f95b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 635kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_286a6caf2a834da8b9ee52b948617a70"
          }
        },
        "d1334e3d8d2048bead2389d0e507a44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dc553df3b1e4ae0a58fd9cc412d7411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15372b33d6c04882a9740a8e8458f95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "286a6caf2a834da8b9ee52b948617a70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentK1991/BERT_summarization_1/blob/master/Copy_of_BERTandGPT2_abstractive_summarization_Apr28_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orLGNduqZ99m",
        "colab_type": "text"
      },
      "source": [
        "# Generating a summary of COVID19 publication\n",
        "\n",
        "This notebook introduces a NLP approach to summarize a and paraphrase a scientific publication. The model is specifically trained on COVID19 related data released as part of the COVID-19 Open Research Dataset Challenge on [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n",
        "\n",
        "The strategy we'll be using here involve\n",
        "1. extract sentences using BERT + clustering\n",
        "2. extract keyword tokens from the extracted sentences using BERT fine-tuned for token classification\n",
        "3. generate a paraphrases from the extracted keywords using GPT2 fine-tuned for making abstractive summarization from keywords\n",
        "\n",
        "The fine-tuning is already done, so we will load the model weights to perform the task. GPU is not necessary for this task,but it should help speed things up a bit, especially at the GPT2 sentence generation step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSNIdwKbZxE5",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "04ae362b-85c4-45ad-d4e5-b0131667837d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "#@title Setup Environment and helper function\n",
        "#@markdown Pip install Huggingface transformers\n",
        "\n",
        "#@markdown if cuda is available, set device = 'cuda'\n",
        "\n",
        "#@markdown setup pytorch environment\n",
        "\n",
        "!pip install transformers==2.6.0\n",
        "\n",
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel, DistilBertModel, DistilBertTokenizer, BertTokenizer, BertForTokenClassification\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "%tensorflow_version 1.x\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "import torch\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.3)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.12.47)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 61.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.38.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.14.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers==2.6.0) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers==2.6.0) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=be84c514012d0141b5db658b3bd22e7d414945f6829d49edb412f06a6caab297\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.6.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1df8cf1fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh1-R8OvnNhv",
        "colab_type": "code",
        "outputId": "418fab19-6690-43fc-ccf2-b254d819d461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#@title change to directory\n",
        "\n",
        "#@markdown change directory to where to models are kept\n",
        "#@markdown make sure this dir contain sub dirs for fine-tuned BERT and GPT2 models\n",
        "\n",
        "%cd '/content/drive/My Drive/Colab Notebooks/BERT_GPT2_summarization'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/BERT_GPT2_summarization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4VsJ6-1YVuk",
        "colab_type": "text"
      },
      "source": [
        "# A bit about the model\n",
        "\n",
        "1. For sentence extraction, we can just use pre-trained distil-bert. this will speed things up even faster.\n",
        "\n",
        "2. For token classification, we will be using BERT-based-cased\n",
        "\n",
        "3. For the GPT2 model, we'll be using GPT2DoubleHead model. The \"DoubleHead\" of the model means the model is trained on both language modeling and multiple choice sentence prediction, and outputs 2 losses, the LM loss or language modeling loss, and MC loss or multiple choice loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuxbGgz_cubC",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "d9a085e2-252d-48a2-8383-ecc0335b691a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#@title Choose Model Config and Weights\n",
        "\n",
        "#@markdown Distil version is fine for this task\n",
        "BERT_pretrained_weights = 'distilbert-base-uncased' #@param [\"distilbert-base-uncased\", \"bert-base-uncased\", \"bert-base-cased\"] {allow-input: true}\n",
        "\n",
        "#@markdown for token classification we used \n",
        "BERTforTokenClassification_config_directory = 'BERT_dir' #@param {type:\"string\"}\n",
        "token_label_files = 'BERT_dir/POS2idx.json' #@param {type:\"string\"}\n",
        "\n",
        "GPT2_config_directory = 'GPT2_dir' #@param {type:\"string\"}\n",
        "\n",
        "print('which BERT pre-trained ? ',BERT_pretrained_weights)\n",
        "print('where is BERT token classifier dir ? ',BERTforTokenClassification_config_directory)\n",
        "print('where is GPT2 dir ? ',GPT2_config_directory)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "which BERT pre-trained ?  distilbert-base-uncased\n",
            "where is BERT token classifier dir ?  BERT_dir\n",
            "where is GPT2 dir ?  GPT2_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qX6HhrUfQPQ",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "45facb2b-33bf-4e64-8af9-7ee678780d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "da8d1c53d71e44dabb130fcdb4b85253",
            "ef22ba1a71d747ccad770b27afd3a60b",
            "ab1f2ff9b3f8408b912f5bfc0b410bc2",
            "30222c3cb41c47c9b7c0ab5aff02ff71",
            "16e8062727654fb2849d9c00a3a210ef",
            "d7a8e07c934546dea428873d95cacba2",
            "1e89829843ba417ca66c9ed79cb75a4a",
            "9eae5e0e932e4d4382916c19523b698c",
            "7ecf7dc9b01e463580e99850600f429c",
            "4bca4ef8234a4543bd4d6f7ef127e579",
            "fe59bb8422da43a8b46870e120638268",
            "7257d84b48664262a1e372af2176de7c",
            "22ecbf34bf6d42b7a25f0dce0f470466",
            "8c11cfcd199346fbb6f8cd2c25194f0b",
            "f69dc0cef880427097808227a3b68069",
            "d80540b5a04b41b8ab9c9852d15d79fb",
            "8be1ccca11094c5981d9765a0c0ca568",
            "12261b92cbbd492ab8cb585ffaaf7a84",
            "73f63f91b5d04758b3160ee0e63a6db4",
            "ac5390f0a3184b35889479d450a6a435",
            "d1334e3d8d2048bead2389d0e507a44a",
            "4dc553df3b1e4ae0a58fd9cc412d7411",
            "15372b33d6c04882a9740a8e8458f95b",
            "286a6caf2a834da8b9ee52b948617a70"
          ]
        }
      },
      "source": [
        "#@title Load models and tokenizers\n",
        "#@markdown the models are big, these may take a few mins, read [here](https://huggingface.co/transformers/serialization.html) for more information\n",
        "\n",
        "print('----loading pre-trained BERT----')\n",
        "BERT_pretrained = DistilBertModel.from_pretrained(\n",
        "                  BERT_pretrained_weights)\n",
        "tokenizer_pretrained = DistilBertTokenizer.from_pretrained(\n",
        "                  BERT_pretrained_weights)\n",
        "print('----loading token labels----')\n",
        "with open(token_label_files, 'r') as fp:\n",
        "    POS2idx = json.load(fp)\n",
        "\n",
        "POS_values = list(POS2idx.keys())\n",
        "print('----loading BERT token classifier----')\n",
        "BERT_token_classifier = BertForTokenClassification.from_pretrained(\n",
        "                      BERTforTokenClassification_config_directory)\n",
        "tokenizer_token_classifier = BertTokenizer.from_pretrained(\n",
        "                      BERTforTokenClassification_config_directory)\n",
        "#BERT_token_classifier.load_state_dict(torch.load(BERTforTokenClassification_finetuned_weights))\n",
        "print('----loading GPT2 summary generator----')\n",
        "tokenizer_GPT2 = GPT2Tokenizer.from_pretrained(\n",
        "                  GPT2_config_directory)\n",
        "special_tokens = {'bos_token':'<|startoftext|>','eos_token':'<|endoftext|>','pad_token':'<pad>','additional_special_tokens':['<|keyword|>','<|summarize|>']}\n",
        "tokenizer_GPT2.add_special_tokens(special_tokens)\n",
        "GPT2_generator = GPT2DoubleHeadsModel.from_pretrained(\n",
        "                  GPT2_config_directory)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----loading pre-trained BERT----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da8d1c53d71e44dabb130fcdb4b85253",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=442, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ecf7dc9b01e463580e99850600f429c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=267967963, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8be1ccca11094c5981d9765a0c0ca568",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----loading token labels----\n",
            "----loading BERT token classifier----\n",
            "----loading GPT2 summary generator----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stFkRaRIw3Z-",
        "colab_type": "code",
        "outputId": "6c520902-a373-417b-b945-bcfc2cd2e9c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "#@title use GPU?\n",
        "\n",
        "#@markdown check the box to indicate if GPU to be used for running any model?\n",
        "\n",
        "use_GPU_BERT_pre_trained = False #@param {type:\"boolean\"}\n",
        "use_GPU_BERT_token_classifier = False #@param {type:\"boolean\"}\n",
        "use_GPU_GPT_generator = True #@param {type:\"boolean\"}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('cuda is available')\n",
        "  device = 'cuda'\n",
        "  print('device is set to cuda')\n",
        "if not torch.cuda.is_available():\n",
        "  print('cuda is not available')\n",
        "  device = 'cpu'\n",
        "  print('device is set to cpu')\n",
        "  use_GPU_BERT_pre_trained = False\n",
        "  use_GPU_BERT_token_classifier = False\n",
        "  use_GPU_GPT_generator = False\n",
        "\n",
        "print(' ')\n",
        "print('use GPU for pre-trained BERT?' ,use_GPU_BERT_pre_trained)\n",
        "print('use GPU for BERT token classifier ?' ,use_GPU_BERT_token_classifier)\n",
        "print('use GPU for GPT2?' ,use_GPU_GPT_generator)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is available\n",
            "device is set to cuda\n",
            " \n",
            "use GPU for pre-trained BERT? False\n",
            "use GPU for BERT token classifier ? False\n",
            "use GPU for GPT2? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fondu_aEjMKo",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "68d95916-c039-40dd-a32b-a7f16cb7d887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#@title Main text file\n",
        "\n",
        "#@markdown indicate the text file to be summarized\n",
        "use_input_text = False\n",
        "\n",
        "input_file = 'directory/file.txt' #@param {type:\"string\"}\n",
        "max_len = 500 #@param {type:\"integer\",max:512}\n",
        "\n",
        "#@markdown or copy paste your input here and check the box\n",
        "use_input_text = True #@param {type:\"boolean\"}\n",
        "input_text = \"'Two months after it was firstly reported, the novel coronavirus disease COVID-19 has already spread worldwide. However, the vast majority of reported infections have occurred in China. To assess the effect of early travel restrictions adopted by the health authorities in China, we have implemented an epidemic metapopulation model that is fed with mobility data corresponding to 2019 and 2020. This allows to compare two radically different scenarios, one with no travel restrictions and another in which mobility is reduced by a travel ban. Our findings indicate that i) travel restrictions are an effective measure in the short term, however, ii) they are ineffective when it comes to completely eliminate the disease. The latter is due to the impossibility of removing the risk of seeding the disease to other regions. Our study also highlights the importance of developing more realistic models of behavioral changes when a disease outbreak is unfolding.'\" #@param {type:\"string\"}\n",
        "\n",
        "if not use_input_text:\n",
        "  # open the txt file that is included\n",
        "  with open(input_file, 'r') as file:\n",
        "    input_text = file.read().replace('\\n', '')\n",
        "\n",
        "# split text to sentences\n",
        "paragraph_split = sent_tokenize(input_text)\n",
        "\n",
        "print('input text has',len(paragraph_split) ,'sentences.')\n",
        "\n",
        "print('tokenizing sentences')\n",
        "\n",
        "input_tokens = []\n",
        "for i in paragraph_split:\n",
        "  input_tokens.append(tokenizer_pretrained.encode(i, \n",
        "                              add_special_tokens=True))\n",
        "temp = []\n",
        "for i in input_tokens:\n",
        "  temp.append(len(i))\n",
        "if np.max(temp) > max_len:\n",
        "  raise ValueError('sentence longer than the max_len')\n",
        "if np.max(temp) > 512:\n",
        "  print('warning: sentence longer than 512')\n",
        "  print('suggest to change max_len to 512, the remainder will be truncated')\n",
        "input_ids = pad_sequences(input_tokens, \n",
        "                          maxlen=max_len, dtype=\"long\", \n",
        "                          value=0, \n",
        "                          truncating=\"post\", \n",
        "                          padding=\"post\")\n",
        "\n",
        "print('creating attention masks')\n",
        "\n",
        "attention_masks = []\n",
        "for sent in input_ids:\n",
        "  att_mask = [int(token_id > 0) for token_id in sent]  # create a list of 0 and 1.\n",
        "  attention_masks.append(att_mask)  # basically attention_masks is a list of list\n",
        "\n",
        "input_ids = torch.tensor(input_ids)  \n",
        "attention_mask = torch.tensor(attention_masks)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input text has 7 sentences.\n",
            "tokenizing sentences\n",
            "creating attention masks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gzatCRwZq_b",
        "colab_type": "text"
      },
      "source": [
        "# More on unsupervised BERT embedding\n",
        "\n",
        "Similar to word2vec, we use BERT to convert sentences in English to a vector that captures semantic relationships (i.e. distance among vectors represents semantic dis-similarity). But unlike word2vec, BERT allows us to do more than just a word in isolation. BERT allows us to a long sequence (up to 512 tokens), enabling us to get a semantic information of a long sequence.\n",
        "\n",
        "To get a sentence level information, we output the last-layer hidden representation of the sentence header token (in BERT this is [CLS]).\n",
        "\n",
        "Then we perform k-means clustering to see the clustering of semantic information into k clusters. The assumption is that each cluster represents a related semantic ideas expressed in the text. Then we extract the sentence closest to the cluster center as representing semantic meaning of the cluster. This sentence becomes *the extractive summary*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWdvPcXimYLi",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "eb9889ff-1ae1-4f01-a7f6-7f124fcfd1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#@title Extracting parameters\n",
        "\n",
        "#@markdown make sure that the number_extract < number of sentences in input text\n",
        "number_extract = 6 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "if use_GPU_BERT_pre_trained:\n",
        "  input_ids = input_ids.to(device)\n",
        "  BERT_pretrained = BERT_pretrained.to(device)\n",
        "  attention_mask = attention_mask.to(device)\n",
        "\n",
        "if not use_GPU_BERT_pre_trained:\n",
        "  input_ids = input_ids.to('cpu')\n",
        "  BERT_pretrained = BERT_pretrained.to('cpu')\n",
        "  attention_mask = attention_mask.to('cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "  last_hidden_states = BERT_pretrained(input_ids, \n",
        "                             attention_mask=attention_mask)\n",
        "\n",
        "sentence_features = last_hidden_states[0][:,0,:].detach().cpu().numpy()\n",
        "\n",
        "print('performing k-medoid clustering with '\n",
        "        ,number_extract,' clusters')\n",
        "\n",
        "kmeans = KMeans(n_clusters=number_extract, \n",
        "                random_state=0).fit(sentence_features)\n",
        "cluster_center = kmeans.cluster_centers_\n",
        "nbrs = NearestNeighbors(n_neighbors= 1, \n",
        "                        algorithm='brute').fit(sentence_features)\n",
        "distances, indices = nbrs.kneighbors(\n",
        "                  cluster_center.reshape(number_extract,-1))\n",
        "\n",
        "\n",
        "indices = np.sort(indices.reshape(1,-1))\n",
        "topic_answer = []\n",
        "# for i in range(len(indices)):\n",
        "#   topic_i = []\n",
        "#   for j in indices[i]:\n",
        "#     topic_i.append(paragraph_split[j])\n",
        "#   topic_answer.append(topic_i)\n",
        "\n",
        "for i in indices[0]:\n",
        "  topic_answer.append(paragraph_split[i])\n",
        "\n",
        "print('result:')\n",
        "\n",
        "print('the ',number_extract,' extracted sentences are')\n",
        "for i in topic_answer:\n",
        "  print(i)\n",
        "\n",
        "topic_answer_string = ''\n",
        "for topic in topic_answer:\n",
        "  topic_answer_string = topic_answer_string + ' '+ topic"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "performing k-medoid clustering with  6  clusters\n",
            "result:\n",
            "the  6  extracted sentences are\n",
            "'Two months after it was firstly reported, the novel coronavirus disease COVID-19 has already spread worldwide.\n",
            "However, the vast majority of reported infections have occurred in China.\n",
            "To assess the effect of early travel restrictions adopted by the health authorities in China, we have implemented an epidemic metapopulation model that is fed with mobility data corresponding to 2019 and 2020.\n",
            "This allows to compare two radically different scenarios, one with no travel restrictions and another in which mobility is reduced by a travel ban.\n",
            "The latter is due to the impossibility of removing the risk of seeding the disease to other regions.\n",
            "Our study also highlights the importance of developing more realistic models of behavioral changes when a disease outbreak is unfolding.'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwCNGlVVrFZQ",
        "colab_type": "code",
        "outputId": "bac9f4e5-e2b8-4f45-a283-32fa807a1b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "#@title Read the Extracted summary\n",
        "wrapper.wrap(topic_answer_string)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" 'Two months after it was firstly reported, the novel coronavirus\",\n",
              " 'disease COVID-19 has already spread worldwide. However, the vast',\n",
              " 'majority of reported infections have occurred in China. To assess the',\n",
              " 'effect of early travel restrictions adopted by the health authorities',\n",
              " 'in China, we have implemented an epidemic metapopulation model that is',\n",
              " 'fed with mobility data corresponding to 2019 and 2020. This allows to',\n",
              " 'compare two radically different scenarios, one with no travel',\n",
              " 'restrictions and another in which mobility is reduced by a travel ban.',\n",
              " 'The latter is due to the impossibility of removing the risk of seeding',\n",
              " 'the disease to other regions. Our study also highlights the importance',\n",
              " 'of developing more realistic models of behavioral changes when a',\n",
              " \"disease outbreak is unfolding.'\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGdUnRyaZRzK",
        "colab_type": "text"
      },
      "source": [
        "# More on fine-tuning the token classification\n",
        "\n",
        "To explore on how to get an abstractive summarization, one approach is to further break down the extracted summary to keyword level, and train a generative model to generate a new sentence from the keywords.\n",
        "\n",
        "But first to get a keyword, we will use BERT that is fine-tuned for part of speech tagging. We will then extract the noun tokens and verb tokens from the sentences, and discard other words or parts of speech, such as adjectives, adverbs, determinants, etc. \n",
        "\n",
        "The transfer learning of BERT is done using an apparoch adapted from this [blogpost](https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/). The training dataset is obtained from [Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). \n",
        "\n",
        "After training BERT for token classification on this dataset for 3 epochs, the validation accuracy is 0.99 and F1-score is 0.93.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC2uQpuinthR",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "8db40774-44e3-4886-c2ca-51fc8077df9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "#@title Keyword extraction\n",
        "\n",
        "list_to_pick = ['NN','NNP','NNPS','NNS','VBD','VB','VBZ','VBP']\n",
        "\n",
        "tokenized_sentence = tokenizer_token_classifier.encode(\n",
        "                      topic_answer_string)\n",
        "input_ids2 = torch.tensor([tokenized_sentence[:510]])\n",
        "\n",
        "if use_GPU_BERT_token_classifier:\n",
        "  BERT_token_classifier = BERT_token_classifier.to(device)\n",
        "  input_ids2 = input_ids2.to(device)\n",
        "\n",
        "if not use_GPU_BERT_token_classifier:\n",
        "  BERT_token_classifier = BERT_token_classifier.to('cpu')\n",
        "  input_ids2 = input_ids2.to('cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "  output2 = BERT_token_classifier(input_ids2)\n",
        "label_indices = np.argmax(output2[0].to('cpu').numpy(), axis=2)\n",
        "\n",
        "list_keywords = []\n",
        "\n",
        "tokens = tokenizer_token_classifier.convert_ids_to_tokens(\n",
        "                        input_ids2.to('cpu').numpy()[0])\n",
        "new_tokens, new_labels = [], []\n",
        "for token, label_idx in zip(tokens, label_indices[0]):\n",
        "    if token.startswith(\"##\"):\n",
        "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "    else:\n",
        "        new_labels.append(POS_values[label_idx])\n",
        "        new_tokens.append(token)\n",
        "for token, label in zip(new_tokens, new_labels):\n",
        "    if label in list_to_pick:\n",
        "      list_keywords.append(token)\n",
        "\n",
        "print('finished keyword extraction ...')\n",
        "print('the keywords are')\n",
        "\n",
        "list_keywords = [i for i in list_keywords if i not in ['[CLS]','[SEP]','?','/','-','.','_','!','@','[',']']]\n",
        "list_keywords\n",
        "\n",
        "list_keywords_str = ' '.join(list_keywords)\n",
        "wrapper.wrap(list_keywords_str)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished keyword extraction ...\n",
            "the keywords are\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"' months was coronavirus disease covid has worldwide majority\",\n",
              " 'infections have china assess effect travel restrictions health',\n",
              " 'authorities china have epidemic metapopulation model is mobility data',\n",
              " 'allows compare scenarios travel restrictions mobility is travel ban is',\n",
              " 'impossibility risk disease regions study highlights importance models',\n",
              " 'changes disease outbreak is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqvcR3YhZcXF",
        "colab_type": "text"
      },
      "source": [
        "# More on OpenAI GPT-2\n",
        "\n",
        "Unlike BERT which is bi-directional encoder, openAI-GPT2 is auto-regressive decoder. (BERT stands for Bidirectional Encoder Representations from Transformers, and GPT stands for Generative Pretrained Transformer). The illustration [here](http://jalammar.github.io/illustrated-gpt2/) is helpful. We will use GPT2 that is specifically fine-tuned for making a summary from keywords.\n",
        "\n",
        "But first, how is the training done?\n",
        "\n",
        "We use GPT2DoubleHead model, meaning, it has 2 heads: one head for causal language modeling (lm), and the other for multiple choice (mc) answering.So for training we have 2 losses to optimize, one is lm loss and the other one is mc loss. \n",
        "\n",
        "The causal language modeling head takes input up to the current tokens and output the next predicted token. The \n",
        "\n",
        "The rationale is that by optimizing two losses, we will force the model to learn both local context used to generate a next token and global semantic meaning for answering multiple choice question. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The illustration of what the training looks like. We create a special token '<|summarize|>' that tells the model to start summarizing using information frm previous tokens up to that point. \n",
        "\n",
        "The training set looks like this:\n",
        "\n",
        "$<|startoftext|> k_1, k_2, k_3, ..., k_n <|summarize|> gold summary...<|endoftext|><pad><pad><pad>... $\n",
        "\n",
        "where $k_n$ is $n^{th}$ keyword of the summary;\n",
        "\n",
        "and $<pad>$ is a token for padding the sequences in the dataset to the same length.\n",
        "\n",
        "We have a distractor that contain the same keywords but a wrong summary. $<|startoftext|> k_1, k_2, k_3, ..., k_n <|summarize|> distractor ...<|endoftext|><pad><pad><pad>... $ \n",
        "\n",
        "The LM loss is the cross-entropy loss computed on the sequence generated after the token <|summarize|> compared to the gold summary.\n",
        "\n",
        "The MC loss is the cross-entropy loss of classifying the gold summary among distractors.\n",
        "\n",
        "---\n",
        "\n",
        "## training of the model\n",
        "\n",
        "we train the model, using 1 Tesla P100-PCIE GPU. The model was trained for 2 epochs. This takes over 8 hours of GPU time. The batch size is 1. To alleviate the problem, we accumulate gradient for 5 iterations before doing gradient descent step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYQjs_jOW2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_keywords_str2 = 'Pathogenesis of Virus-Induced DemyelinationPublisher Summary Demyelination is component diseases humans are sclerosing panencephalitis SSPE leukoencephalopathy PML are number virus infections animals involve demyelination serve models demyelinating diseases diseases viruses have be demyelination situations demyelinating disease chapter reviews architecture organization CNS considers is interaction viruses CNS cells discusses immunology CNS differs aspects rest body models demyelination have Viruses disease have features include RNA viruses viruses chapter attempts summarize factors demyelination their features mechanisms'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xSXM3PKVpSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title = 'A data-driven assessment of early travel restrictions related to the spreading of the novel COVID-19 within mainland China '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmxxI7XyqO-_",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "efafc9ef-8b20-407d-c49e-3bd80062aa57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "#@title GPT2 input preparation\n",
        "\n",
        "GPT2_input = tokenizer_GPT2.encode(\n",
        "      '<|startoftext|> ' +title + list_keywords_str + ' <|summarize|> ')\n",
        "GPT2_input_torch = torch.tensor(GPT2_input, dtype=torch.long)\n",
        "\n",
        "print(\"the keyword input :\")\n",
        "wrapper.wrap(tokenizer_GPT2.decode(GPT2_input_torch))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the keyword input :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|startoftext|>  A data-driven assessment of early travel restrictions',\n",
              " 'related to the spreading of the novel COVID-19 within mainland',\n",
              " \"China'months was coronavirus disease covid has worldwide majority\",\n",
              " 'infections have china assess effect travel restrictions health',\n",
              " 'authorities china have epidemic metapopulation model is mobility data',\n",
              " 'allows compare scenarios travel restrictions mobility is travel ban is',\n",
              " 'impossibility risk disease regions study highlights importance models',\n",
              " 'changes disease outbreak is <|summarize|>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDJ2UdZeZga2",
        "colab_type": "text"
      },
      "source": [
        "# More on Sentence generation\n",
        "\n",
        "To generate a new sentence, we pass the input to the model, the output is the likelihood distribution of tokens.\n",
        "\n",
        "To get the token from the output, we apply softmax activation to the output. Then we have the word for that time step. To get a sequence of tokens, we repeat this step, passing the output back in as input.\n",
        "\n",
        "Now, there are many ways to generate a long sentence, one is greedy search, which always choose the next word as highest probability given previous words.\n",
        "\n",
        "## greedy search\n",
        "\n",
        "This is not always the best idea, because the correct word could have showed up behind a low probability word. The greedy search, by rejecting the low probability word, also reject the correct word that comes after. Note that this method is deterministic.\n",
        "\n",
        "## top-k sampling/ top-p sampling\n",
        "\n",
        "This is to pick next words according to its conditional probability on previous words. So this is not deterministic anymore. \n",
        "\n",
        "- Temperature is a scaling factor (always positive real number) apply to the likelihood before softmax. higher temperature (>1) shrink all the likelihood together, making high likelihood and low likelihood closer; this result in word sequence appear more random (and sounds creative) than before. Low temperature sharpening the distribution, increasing the high likelihood and decreasing the likelihood of low probability word, making the result more deterministic. \n",
        "- top-k = the top k word candidates to consider when doing the sampling.\n",
        "For example, top-k = 5 will consider top 5 words when doing the sampling. Higher top-k value means considering many low probability words, thus, there is some chance that these words will be used.\n",
        "\n",
        "- top-p = top p sampling choose top n word candidates such that the set of these n words have cumulative probability > p. For example, if we set p = 0.5 it might choose top 5 words, if the sum of the probability of these 5 words > 0.5. Next round it might choose different 6 words, ... etc. This allows more dynamic sampling there n cab vary depending on the cumulative probability.\n",
        "\n",
        "- using both top-k and top-p sampling together allows us to cap the number of n to be <= k. This allows us to limit how many words we will consider in case the words all have low probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh5mwyTpqPH6",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "cd96baa5-2813-42ec-87b5-280d8417d605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#@title GPT2 paraphrase generation\n",
        "\n",
        "#@markdown this step may takes a few mins without GPU\n",
        "\n",
        "temperature =  1#@param {type:\"number\"}\n",
        "greedy_search = False #@param {type:\"boolean\"}\n",
        "top_k =  50#@param {type:\"integer\",min:1}\n",
        "top_p = 0.8 #@param {type:\"number\",max:1}\n",
        "max_length = 200 #@param {type:\"integer\",max:1}\n",
        "\n",
        "min_length= 20 #@param {type:\"integer\",max:1}\n",
        "num_return_sequences=3 #@param {type:\"integer\",min:1}\n",
        "\n",
        "if use_GPU_GPT_generator:\n",
        "  GPT2_generator = GPT2_generator.to(device)\n",
        "  GPT2_input_torch = GPT2_input_torch.to(device)\n",
        "\n",
        "do_sample = not greedy_search\n",
        "if do_sample == False:\n",
        "  num_return_sequences = 1\n",
        "  \n",
        "sampling_output = GPT2_generator.generate(\n",
        "      input_ids=GPT2_input_torch.unsqueeze(0),\n",
        "      max_length=max_length + len(GPT2_input_torch),\n",
        "      min_length = min_length + len(GPT2_input_torch),\n",
        "      temperature=temperature,\n",
        "      decoder_start_token_id= '<|summarize|>',\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      do_sample=do_sample,\n",
        "      num_return_sequences=num_return_sequences, \n",
        "      no_repeat_ngram_size=3)\n",
        "\n",
        "print('finish generating')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish generating\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOAS0DBxrdKw",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "fa7aa141-5b57-4b24-e99a-de851ada52c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "#@title GPT2 generated output\n",
        "\n",
        "which_output = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "wrapper.wrap(tokenizer_GPT2.decode(\n",
        "    sampling_output[which_output,len(GPT2_input_torch):], \n",
        "    skip_special_tokens=True)[:5000])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Abstract In order to assess the impact of travel restrictions, global',\n",
              " 'health authorities, including the affected china, have established an',\n",
              " 'epidemic metaperopulation. A multi-species model is developed and',\n",
              " 'applied to the mobility of the data, which allows us to compare the',\n",
              " 'scenarios of travel restriction, mobility, and is safe. The rapid',\n",
              " 'travel ban that is proposed is not without a great obstacle to the',\n",
              " 'disease regions of this study. Here, we highlight the importance of',\n",
              " 'multiple models of the epidemiological changes in disease outbreak and',\n",
              " 'is presented in order to compare in our scenarios with travel',\n",
              " 'restrictions and mobility. The epidemiological mobility is critical',\n",
              " 'for predicting travel ban, which is feasible. It is difficult to risk',\n",
              " 'a disease in these regions. In this study, we also highlights the',\n",
              " 'importance and potential of multiple and distinct models of regional',\n",
              " 'and international changes in the disease outbreak, which it is well',\n",
              " 'known that the most effective model is the mobility and the data',\n",
              " 'allows us and our researchers to compare and compare the different',\n",
              " 'scenarios of the travel']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdb8x5poRoco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gold_label = 'Two months after it was firstly reported, the novel coronavirus disease COVID-19 has already spread worldwide. However, the vast majority of reported infections have occurred in China. To assess the effect of early travel restrictions adopted by the health authorities in China, we have implemented an epidemic metapopulation model that is fed with mobility data corresponding to 2019 and 2020. This allows to compare two radically different scenarios, one with no travel restrictions and another in which mobility is reduced by a travel ban. Our findings indicate that i) travel restrictions are an effective measure in the short term, however, ii) they are ineffective when it comes to completely eliminate the disease. The latter is due to the impossibility of removing the risk of seeding the disease to other regions. Our study also highlights the importance of developing more realistic models of behavioral changes when a disease outbreak is unfolding.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joWup4t6RtnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "3e8f3e3d-35ec-4945-923b-0636c8dcf1a1"
      },
      "source": [
        "wrapper.wrap(title + gold_label)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A data-driven assessment of early travel restrictions related to the',\n",
              " 'spreading of the novel COVID-19 within mainland China Two months after',\n",
              " 'it was firstly reported, the novel coronavirus disease COVID-19 has',\n",
              " 'already spread worldwide. However, the vast majority of reported',\n",
              " 'infections have occurred in China. To assess the effect of early',\n",
              " 'travel restrictions adopted by the health authorities in China, we',\n",
              " 'have implemented an epidemic metapopulation model that is fed with',\n",
              " 'mobility data corresponding to 2019 and 2020. This allows to compare',\n",
              " 'two radically different scenarios, one with no travel restrictions and',\n",
              " 'another in which mobility is reduced by a travel ban. Our findings',\n",
              " 'indicate that i) travel restrictions are an effective measure in the',\n",
              " 'short term, however, ii) they are ineffective when it comes to',\n",
              " 'completely eliminate the disease. The latter is due to the',\n",
              " 'impossibility of removing the risk of seeding the disease to other',\n",
              " 'regions. Our study also highlights the importance of developing more',\n",
              " 'realistic models of behavioral changes when a disease outbreak is',\n",
              " 'unfolding.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnT8xaE4I-cp",
        "colab_type": "text"
      },
      "source": [
        "# conclusion\n",
        "\n",
        "1. The extractive method really work well. From eye-ball sampling, what we have is really human-level performance. This is mainly because we extract text from human-written text, without modification. So the human touch is still there.\n",
        "\n",
        "  - It is most useful especially when we do not have sample data to train the abstractive summarization model. In domain-specific or highly technical text, the amount of training data may be limited. But since this method is unsupervised learning, it should not be affected by this limitation as long as it can perform the embedding well.\n",
        "\n",
        "  - However, it has limitation in that it cannot generate a paraphrase. Sometimes, this can lead to awkward or clumpsy result that looks like a copy-paste extraction.\n",
        "\n",
        "\n",
        "2. The abstractive method is doing OK, and generally generate a text that sounds like coming from a correct topic. And generally, one can understand what it tries to say. But the performance is really not at the human level performance. \n",
        "\n",
        "  - For one, the abstractive method is limited by the amount of training resource, namely the GPU time and the labeled data. We only have 32.1K text to train on a highly technical domain. So it's very difficult for the generative model to learn the relationship between these technical terms from this small set. \n",
        "\n",
        "  - it often makes syntactical blunder, like repeating itself is a big problem. This has to do with the auto-regressive model, feeding generated output back as a new input and this feedback is susceptible to a kind of vicious circle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_mSouJZjeI",
        "colab_type": "text"
      },
      "source": [
        "# Citation\n",
        "\n",
        "[COVID19 Open Research Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n",
        "- dataset of peer-reviewed medical journals used to do the fine-tuning of GPT2 model\n",
        "\n",
        "[Huggingface](https://github.com/huggingface/transformers)\n",
        "- repository of pytorch-based NLP models.\n",
        "\n",
        "[BERT for first-time users](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n",
        "- blogpost on how BERT embedding works.\n",
        "\n",
        "[Named Entity Recognition with BERT](https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/)\n",
        "- blogpost on fine-tuning BERT model for token classification. The fine-tuning of BERT for token classification is based on this blogpost.\n",
        "\n",
        "[Annotated-corpus for Named entity recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus#ner_dataset.csv)\n",
        "- dataset containing labeled tokens for training NLP models on token classification.\n",
        "\n",
        "[The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/)\n",
        "- a blogpost illustrating how GPT-2 works. \n",
        "\n",
        "[GPT2 Double Head Model](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)\n",
        "- a blogpost on how the Double Head model works. The training of GPT2 is adapted from this blogpost.\n",
        "\n",
        "[Different decoding method for language generation](https://huggingface.co/blog/how-to-generate)\n",
        "- a blogpost on how language generation works, including explanation on greedy search, top-k and top-p sampling. "
      ]
    }
  ]
}